{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da1849-b835-4404-8090-b869095cc180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from tqdm.auto import tqdm\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Seaborn theme and font scaling for better aesthetics\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\", {\n",
    "    \"grid.linestyle\": \":\", \n",
    "    \"border.color\": \"black\",\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"xtick.bottom\": \"True\",\n",
    "    \"xtick.top\": \"True\",\n",
    "    \"ytick.left\": \"True\",\n",
    "    \"ytick.right\": \"True\",\n",
    "    \"xtick.direction\": \"in\",\n",
    "    \"ytick.direction\": \"in\"\n",
    "})\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981c1a2-5c8b-4ed4-83ae-c8623ac07818",
   "metadata": {},
   "source": [
    "# function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69acc01-faf1-4c71-897c-1ffbb503f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def dict_configs(d, return_dict=False):\n",
    "    for k, v in d.items():\n",
    "        if not hasattr(v, '__len__'):\n",
    "            d[k] = [v]\n",
    "    Config = namedtuple('Config', d.keys())\n",
    "    for vcomb in product(*d.values()):\n",
    "        c = dict(zip(d.keys(), vcomb))\n",
    "        if return_dict:\n",
    "            yield Config(**c), c\n",
    "        else:\n",
    "            yield Config(**c)\n",
    "\n",
    "\n",
    "def BN(X):\n",
    "    X = X - X.mean(dim=0)\n",
    "    X = X / (X ** 2).mean(dim=0) ** 0.5\n",
    "    return X\n",
    "\n",
    "def activation(act='lin'):\n",
    "    if act == 'tanh':\n",
    "        return lambda X: torch.tanh(X)\n",
    "    if act == 'sin':\n",
    "        return lambda X: torch.sin(X)\n",
    "    if act == 'cos':\n",
    "        return lambda X: torch.cos(X)\n",
    "    if act == 'relu':\n",
    "        return lambda X: torch.relu(X)\n",
    "    if act == 'id':\n",
    "        return lambda X: X\n",
    "    try:\n",
    "        return eval(f'torch.{act}')\n",
    "    except:\n",
    "        assert False\n",
    "\n",
    "def gen_input(n, d, distortion=10):\n",
    "    H = torch.randn(n, d)\n",
    "    D = torch.ones(n)\n",
    "    D[0] *= distortion\n",
    "    D = torch.diag(D)\n",
    "    svd = torch.svd(H)\n",
    "    H = svd.U.mm(D.mm(svd.V.t()))\n",
    "    return H\n",
    "\n",
    "def create_input(batch_size, input_size, degeneracy=10):\n",
    "    H = torch.randn(batch_size, input_size)\n",
    "    D = torch.ones(batch_size)\n",
    "    D[0] *= degeneracy\n",
    "    D = torch.diag(D)\n",
    "    svd = torch.svd(H)\n",
    "    H = svd.U.mm(D.mm(svd.V.t()))\n",
    "    return H\n",
    "\n",
    "class CustomBatchNorm1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBatchNorm1d, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=0, keepdim=True)\n",
    "        std = x.std(dim=0, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / std \n",
    "        return x_norm \n",
    "\n",
    "class CustomActivation(nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super(CustomActivation, self).__init__()\n",
    "\n",
    "        if activation == 'sin':\n",
    "            self.activation_fn = torch.sin\n",
    "        elif activation == 'relu':\n",
    "            self.activation_fn = torch.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_fn = torch.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation_fn = torch.sigmoid\n",
    "        elif activation == 'selu':\n",
    "            self.activation_fn = nn.SELU()\n",
    "        elif activation == 'linear':\n",
    "            self.activation_fn = lambda x: x\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation_fn(x)\n",
    "\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, activation='relu', batch_norm=True):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = CustomActivation(activation)\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        if self.batch_norm:\n",
    "            self.layers.append(CustomBatchNorm1d())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            if self.batch_norm:\n",
    "                self.layers.append(CustomBatchNorm1d())\n",
    "            self.layers.append(self.activation)\n",
    "                    \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                gain = torch.nn.init.calculate_gain(activation)\n",
    "                init.xavier_normal_(layer.weight, gain=gain)\n",
    "                init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x, return_hidden=False):\n",
    "        hidden_activations = [x]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            # return activations\n",
    "            if isinstance(layer, nn.Linear): \n",
    "                hidden_activations.append(x)\n",
    "        \n",
    "        if return_hidden:\n",
    "            return hidden_activations[:-1], hidden_activations[-1]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "def marchenko_pastur_pdf(x, Q, sigma=1):\n",
    "    # Q = X/Y where X is number of samples and Y is number of features\n",
    "    # sigma is the standard deviation of the samples.\n",
    "    b = sigma * np.power((1 + np.sqrt(1/Q)),2)\n",
    "    a = sigma * np.power((1 - np.sqrt(1/Q)),2)\n",
    "    pdf = Q**2*(1/(2*np.pi*sigma*sigma*Q*x))*np.sqrt((b-x)*(x-a))*(x>a)*(x<b)\n",
    "    pdf[np.isnan(pdf)] = 0\n",
    "    return pdf\n",
    "    \n",
    "def calc_G_star(matrix):\n",
    "    diagonal_mean = np.mean(np.diag(matrix))\n",
    "    np.fill_diagonal(matrix, diagonal_mean)\n",
    "    off_diagonal_mean = np.mean(matrix[~np.eye(matrix.shape[0], dtype=bool)])\n",
    "    matrix[~np.eye(matrix.shape[0], dtype=bool)] = off_diagonal_mean\n",
    "\n",
    "\n",
    "def run_experiment(G, calc_eigs=False):\n",
    "    df = []\n",
    "    for c, conf in tqdm(list(dict_configs(G, return_dict=True)), leave=False): \n",
    "        hidden_sizes = [c.d] * c.depth\n",
    "        sample_input = create_input(c.n, c.input_size, c.degeneracy)\n",
    "        mlp = CustomMLP(c.input_size, c.output_size, hidden_sizes, c.activation, c.batch_norm)\n",
    "        mlp.to(device)\n",
    "        hidden_layers, output = mlp(sample_input.to(device), return_hidden=True)\n",
    "        for l, X in enumerate(hidden_layers):\n",
    "            G = X @ X.T / c.d\n",
    "            eigs = None\n",
    "            if calc_eigs:\n",
    "                eigs = torch.linalg.eigvalsh(G)\n",
    "                eigs = eigs.detach().cpu().numpy()\n",
    "            df.append({**conf, 'l': l, 'G': G.detach().cpu().numpy(), 'eigs': eigs})\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    G_star = df.groupby(['d', 'n', 'activation', 'depth','batch_norm'])['G'].apply(lambda x: np.mean(np.stack(x), 0)).reset_index()\n",
    "    G_star = G_star.loc[(G_star.d == G_star.d.max()) & (G_star.depth == G_star.depth.max())]\n",
    "    G_star.G.apply(calc_G_star)\n",
    "    df = pd.merge(df, G_star, how='left', on=['n', 'activation','batch_norm'], suffixes=('', '_star'))\n",
    "    df.loc[:, 'G_diff'] = df.G_star - df.G\n",
    "    df.loc[:, 'G_diff_fro'] = df.G_diff.apply(lambda x: np.sum(x**2)**0.5)\n",
    "    df.loc[:, 'G_star_cond'] = df.G_star.apply(lambda x: np.linalg.cond(x))\n",
    "    alpha = 0.5\n",
    "    df.loc[:, 'theory'] = df.apply(lambda x: 2*x.G_star_cond * ((1 - alpha)**(x.l / 2) + 0.25 * x.n / np.sqrt(x.d) / alpha), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_results(df, x, y, estimator, hue=None, linestyle=None, color=None, marker=None, \n",
    "                 xlabel=None, ylabel=None, title=None, log_xscale=False, log_yscale=False, save_path=None):\n",
    "    plt.figure()\n",
    "    sns.lineplot(data=df, x=x, y=y, estimator=estimator, hue=hue, linestyle=linestyle, color=color, marker=marker)\n",
    "    df = df.groupby([x])['theory'].agg(np.median).reset_index()\n",
    "    sns.lineplot(data=df, x=x, y='theory', linestyle='--', color='k', label='theory')\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if log_yscale:\n",
    "        plt.yscale('log')\n",
    "    if log_xscale:\n",
    "        plt.xscale('log')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dict_configs(d, return_dict=False):\n",
    "    for k, v in d.items():\n",
    "        if not hasattr(v, '__len__'):\n",
    "            d[k] = [v]\n",
    "    Config = namedtuple('Config', d.keys())\n",
    "    for vcomb in product(*d.values()):\n",
    "        c = dict(zip(d.keys(), vcomb))\n",
    "        if return_dict:\n",
    "            yield Config(**c), c\n",
    "        else:\n",
    "            yield Config(**c)\n",
    "\n",
    "\n",
    "def BN(X):\n",
    "    X = X - X.mean(dim=0)\n",
    "    X = X / (X ** 2).mean(dim=0) ** 0.5\n",
    "    return X\n",
    "\n",
    "def activation(act='lin'):\n",
    "    if act == 'tanh':\n",
    "        return lambda X: torch.tanh(X)\n",
    "    if act == 'sin':\n",
    "        return lambda X: torch.sin(X)\n",
    "    if act == 'cos':\n",
    "        return lambda X: torch.cos(X)\n",
    "    if act == 'relu':\n",
    "        return lambda X: torch.relu(X)\n",
    "    if act == 'id':\n",
    "        return lambda X: X\n",
    "    try:\n",
    "        return eval(f'torch.{act}')\n",
    "    except:\n",
    "        assert False\n",
    "\n",
    "def gen_input(n, d, distortion=10):\n",
    "    H = torch.randn(n, d)\n",
    "    D = torch.ones(n)\n",
    "    D[0] *= distortion\n",
    "    D = torch.diag(D)\n",
    "    svd = torch.svd(H)\n",
    "    H = svd.U.mm(D.mm(svd.V.t()))\n",
    "    return H\n",
    "\n",
    "def create_input(batch_size, input_size, degeneracy=10):\n",
    "    H = torch.randn(batch_size, input_size)\n",
    "    D = torch.ones(batch_size)\n",
    "    D[0] *= degeneracy\n",
    "    D = torch.diag(D)\n",
    "    svd = torch.svd(H)\n",
    "    H = svd.U.mm(D.mm(svd.V.t()))\n",
    "    return H\n",
    "\n",
    "class CustomBatchNorm1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBatchNorm1d, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=0, keepdim=True)\n",
    "        std = x.std(dim=0, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / std \n",
    "        return x_norm \n",
    "\n",
    "class CustomActivation(nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super(CustomActivation, self).__init__()\n",
    "\n",
    "        if activation == 'sin':\n",
    "            self.activation_fn = torch.sin\n",
    "        elif activation == 'relu':\n",
    "            self.activation_fn = torch.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_fn = torch.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation_fn = torch.sigmoid\n",
    "        elif activation == 'selu':\n",
    "            self.activation_fn = nn.SELU()\n",
    "        elif activation == 'linear':\n",
    "            self.activation_fn = lambda x: x\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation_fn(x)\n",
    "\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, activation='relu', batch_norm=True):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = CustomActivation(activation)\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        if self.batch_norm:\n",
    "            self.layers.append(CustomBatchNorm1d())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            if self.batch_norm:\n",
    "                self.layers.append(CustomBatchNorm1d())\n",
    "            self.layers.append(self.activation)\n",
    "                    \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                gain = torch.nn.init.calculate_gain(activation)\n",
    "                init.xavier_normal_(layer.weight, gain=gain)\n",
    "                init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x, return_hidden=False):\n",
    "        hidden_activations = [x]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            # return activations\n",
    "            if isinstance(layer, nn.Linear): \n",
    "                hidden_activations.append(x)\n",
    "        \n",
    "        if return_hidden:\n",
    "            return hidden_activations[:-1], hidden_activations[-1]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "def marchenko_pastur_pdf(x, Q, sigma=1):\n",
    "    # Q = X/Y where X is number of samples and Y is number of features\n",
    "    # sigma is the standard deviation of the samples.\n",
    "    b = sigma * np.power((1 + np.sqrt(1/Q)),2)\n",
    "    a = sigma * np.power((1 - np.sqrt(1/Q)),2)\n",
    "    pdf = Q**2*(1/(2*np.pi*sigma*sigma*Q*x))*np.sqrt((b-x)*(x-a))*(x>a)*(x<b)\n",
    "    pdf[np.isnan(pdf)] = 0\n",
    "    return pdf\n",
    "    \n",
    "def calc_G_star(matrix):\n",
    "    diagonal_mean = np.mean(np.diag(matrix))\n",
    "    np.fill_diagonal(matrix, diagonal_mean)\n",
    "    off_diagonal_mean = np.mean(matrix[~np.eye(matrix.shape[0], dtype=bool)])\n",
    "    matrix[~np.eye(matrix.shape[0], dtype=bool)] = off_diagonal_mean\n",
    "\n",
    "\n",
    "def run_experiment(G, calc_eigs=False):\n",
    "    df = []\n",
    "    for c, conf in tqdm(list(dict_configs(G, return_dict=True)), leave=False): \n",
    "        hidden_sizes = [c.d] * c.depth\n",
    "        sample_input = create_input(c.n, c.input_size, c.degeneracy)\n",
    "        mlp = CustomMLP(c.input_size, c.output_size, hidden_sizes, c.activation, c.batch_norm)\n",
    "        mlp.to(device)\n",
    "        hidden_layers, output = mlp(sample_input.to(device), return_hidden=True)\n",
    "        for l, X in enumerate(hidden_layers):\n",
    "            G = X @ X.T / c.d\n",
    "            eigs = None\n",
    "            if calc_eigs:\n",
    "                eigs = torch.linalg.eigvalsh(G)\n",
    "                eigs = eigs.detach().cpu().numpy()\n",
    "            df.append({**conf, 'l': l, 'G': G.detach().cpu().numpy(), 'eigs': eigs})\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    G_star = df.groupby(['d', 'n', 'activation', 'depth','batch_norm'])['G'].apply(lambda x: np.mean(np.stack(x), 0)).reset_index()\n",
    "    G_star = G_star.loc[(G_star.d == G_star.d.max()) & (G_star.depth == G_star.depth.max())]\n",
    "    G_star.G.apply(calc_G_star)\n",
    "    df = pd.merge(df, G_star, how='left', on=['n', 'activation','batch_norm'], suffixes=('', '_star'))\n",
    "    df.loc[:, 'G_diff'] = df.G_star - df.G\n",
    "    df.loc[:, 'G_diff_fro'] = df.G_diff.apply(lambda x: np.sum(x**2)**0.5)\n",
    "    df.loc[:, 'G_star_cond'] = df.G_star.apply(lambda x: np.linalg.cond(x))\n",
    "    alpha = 0.5\n",
    "    df.loc[:, 'theory'] = df.apply(lambda x: 2*x.G_star_cond * ((1 - alpha)**(x.l / 2) + 0.25 * x.n / np.sqrt(x.d) / alpha), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_results(df, x, y, estimator, hue=None, linestyle=None, color=None, marker=None, \n",
    "                 xlabel=None, ylabel=None, title=None, log_xscale=False, log_yscale=False, save_path=None):\n",
    "    plt.figure()\n",
    "    sns.lineplot(data=df, x=x, y=y, estimator=estimator, hue=hue, linestyle=linestyle, color=color, marker=marker)\n",
    "    df = df.groupby([x])['theory'].agg(np.median).reset_index()\n",
    "    sns.lineplot(data=df, x=x, y='theory', linestyle='--', color='k', label='theory')\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if log_yscale:\n",
    "        plt.yscale('log')\n",
    "    if log_xscale:\n",
    "        plt.xscale('log')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7186f84-9a38-43ca-815b-52e894ab5175",
   "metadata": {},
   "source": [
    "# Relationship between mean-field error, depth, width, and batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a382c6f-afc1-4cff-9d99-d771e1955574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theory_validation_figs():\n",
    "    # Define the hyperparameter grid\n",
    "    G = {\n",
    "        'd': [50, 100, 200, 500, 1000],\n",
    "        'n': [10, 20, 30, 40, 50],\n",
    "        'activation': ['relu', 'tanh', 'selu'], \n",
    "        'it': range(10),\n",
    "        'degeneracy': [20],\n",
    "        'depth': [20],\n",
    "        'input_size': [1000],\n",
    "        'output_size': [1000],\n",
    "        'batch_norm': [True],\n",
    "    }\n",
    "\n",
    "    # Run the experiment\n",
    "    df = run_experiment(G)\n",
    "    alpha = 0.5\n",
    "    df.loc[:, 'theory'] = df.apply(lambda x: 2*x.G_star_cond * ((1 - alpha)**(x.l / 2) + 0.25 * x.n / np.sqrt(x.d) / alpha), axis=1)\n",
    "\n",
    "    # Plot results\n",
    "    estimator = np.median\n",
    "\n",
    "    df2 = df.loc[(df.n == df.n.min()) & (df.l == df.l.max())]\n",
    "    plot_results(df2, x='d', y='G_diff_fro', \n",
    "                 log_xscale=True, log_yscale=True,\n",
    "                 estimator=estimator, hue='activation', marker='o',\n",
    "                 xlabel='width $d$', ylabel='$\\|G_\\ell-G_*\\|_F$',\n",
    "                 save_path='icml_plots/Gram_diff_vs_d.pdf')\n",
    "\n",
    "    df2 = df.loc[(df.d == df.d.max()) & (df.n == df.n.min())]\n",
    "    df2 = df2.loc[df.l > 1]\n",
    "    plot_results(df2, x='l', y='G_diff_fro', estimator=estimator, hue='activation', marker='o',\n",
    "                 log_yscale=True,\n",
    "                 xlabel='depth $\\ell$', ylabel='$\\|G_\\ell-G_*\\|_F$',\n",
    "                 save_path='icml_plots/Gram_diff_vs_l.pdf')\n",
    "\n",
    "    df2 = df.loc[(df.d == df.d.max()) & (df.l == df.l.max())]\n",
    "    plot_results(df2, x='n', y='G_diff_fro', estimator=estimator, hue='activation', marker='o',\n",
    "                 xlabel='batch size $n$', ylabel='$\\|G_\\ell-G_*\\|_F$',\n",
    "                 log_xscale=True, log_yscale=True,\n",
    "                 save_path='icml_plots/Gram_diff_vs_n.pdf')\n",
    "\n",
    "theory_validation_figs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2eac1-47b0-4eb4-882c-dd56ca2c5de7",
   "metadata": {},
   "source": [
    "# Emprical spectral distribution of hidden representations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d06654-8e7c-4d18-9e03-2cf243b2f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESD_fig():\n",
    "    # Define the hyperparameter grid\n",
    "    G = {\n",
    "        'd': [1000],\n",
    "        'n': [20],\n",
    "        'activation': ['tanh', 'relu'], \n",
    "        'it': range(10),\n",
    "        'degeneracy': [1],\n",
    "        'depth': [20],\n",
    "        'input_size': [1000],\n",
    "        'output_size': [1000],\n",
    "        'batch_norm': [True],\n",
    "    }\n",
    "    \n",
    "    # Run the experiment\n",
    "    df = run_experiment(G, calc_eigs=True)\n",
    "    df2 = df.loc[df.l == df.l.max() - 1].explode('eigs')\n",
    "    df2.eigs = df2.eigs ** 0.5\n",
    "    median_eigs = df2.groupby('activation')['eigs'].median()\n",
    "    df2['eigs_normalized'] = df2.groupby('activation')['eigs'].transform(lambda x: x / median_eigs[x.name])\n",
    "    \n",
    "    # Plot the histogram of normalized eigenvectors\n",
    "    x = np.arange(0, 4, 0.05)\n",
    "    g = sns.FacetGrid(df2, hue='activation', height=5)\n",
    "    g.map(sns.histplot, 'eigs_normalized', stat='density', bins=x, alpha=0.5)\n",
    "    g.set(xlabel='singular values of $h_\\ell$', ylabel='Density')\n",
    "    g.fig.tight_layout()\n",
    "    \n",
    "    # Calculate gamma value\n",
    "    gamma = df.n.iloc[0] / df.d.iloc[0]\n",
    "    pdf = marchenko_pastur_pdf(x, 1.0/gamma)\n",
    "    \n",
    "    # Plot the PDF with dashed line and label\n",
    "    plt.plot(x, pdf, '--', label=f'MP $\\gamma={gamma}$', color='k')\n",
    "    plt.legend()\n",
    "    plt.savefig('icml_plots/singular_values_relu_tanh.pdf', format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_histogram(df, x, colors):\n",
    "    for act in df.act.unique():\n",
    "        plt.figure()\n",
    "        for c, gamma in zip(colors, df.gamma.unique()[:]):\n",
    "            df2 = df.loc[(df.act == act) & (df.gamma == gamma) & (df.l == df.l.max())]\n",
    "            n = df2.n.iloc[0]\n",
    "            d = df2.d.iloc[0]\n",
    "            gamma = df2.gamma.iloc[0]\n",
    "            pdf = marchenko_pastur_pdf(x, 1.0/gamma)\n",
    "            plt.plot(x, pdf, '--', label=f'MP $\\gamma={gamma}$', color=c)\n",
    "            values = df2.H_eig.values\n",
    "            plt.hist(values / np.median(values), color=c, density=True, alpha=0.2, bins=x, label=f'n={n}, d={d}')\n",
    "            plt.title(act)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Singular values $h_\\ell$')\n",
    "        plt.ylabel('Density')\n",
    "        plt.savefig(f'icml_plots/singular_values_{act}.pdf', format='pdf')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def ESD_supplementary():\n",
    "    Num = 1\n",
    "    L = 20\n",
    "    # Lists to store results\n",
    "    all_results = []\n",
    "    df = []\n",
    "\n",
    "    # Loop over activation functions\n",
    "    for act in ['relu', 'tanh', 'sin', 'selu', 'celu', 'sigmoid']:\n",
    "        F = activation(act)\n",
    "\n",
    "        # Loop over different values of d\n",
    "        for d in [1000, 2000, 5000]:\n",
    "            n = 500\n",
    "            gamma = n / d\n",
    "            results = []\n",
    "\n",
    "            for it in range(Num):\n",
    "                # create input \n",
    "                H = torch.randn(L + 1, n, d).to(device)  \n",
    "\n",
    "                sings = []\n",
    "\n",
    "                for l in range(1, L + 1):\n",
    "                    W = torch.randn(d, d).to(device) / np.sqrt(d)\n",
    "                    A = 1 / (d ** 0.5) * F(BN(H[l - 1]))\n",
    "                    S = torch.svd(A, compute_uv=False).S\n",
    "                    sings.append(S.cpu())\n",
    "                    H[l] = A.mm(W)\n",
    "                    HS = torch.svd(H[l], compute_uv=False).S\n",
    "                    for ei, (e, he) in enumerate(zip(S.cpu().numpy(), HS.cpu().numpy())):\n",
    "                        df.append(dict(it=it, n=n, d=d, l=l, act=act, eig_rank=ei, A_eig=e, H_eig=he, gamma=gamma))\n",
    "\n",
    "                sings = torch.stack(sings)\n",
    "                results.append(sings)\n",
    "\n",
    "            results = torch.stack(results)\n",
    "            all_results.append((n, d, gamma, results))\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    plot_histogram(df, x=np.arange(0.01, 7, 0.1), colors=sns.color_palette(\"Dark2\"))\n",
    "\n",
    "\n",
    "ESD_fig()\n",
    "\n",
    "ESD_supplementary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
